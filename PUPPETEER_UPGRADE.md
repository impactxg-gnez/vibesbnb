# Puppeteer Upgrade Guide

If you need to handle scrolling, dynamic content loading, or JavaScript-rendered pages, here's how to upgrade to Puppeteer.

## Why Upgrade to Puppeteer?

Current limitations with Cheerio:
- âŒ Cannot scroll to load more images
- âŒ Cannot execute JavaScript
- âŒ Cannot handle lazy-loaded content
- âŒ Cannot interact with dynamic elements

Puppeteer benefits:
- âœ… Full browser automation
- âœ… Scroll and load all images
- âœ… Handle JavaScript-rendered content
- âœ… Better success rate with modern sites
- âœ… Screenshot capabilities

## Installation

```bash
npm install puppeteer
# or
pnpm add puppeteer
```

For serverless/Vercel:
```bash
npm install chrome-aws-lambda puppeteer-core
```

## Implementation

### Option 1: Full Puppeteer (Local/VPS Hosting)

Create `apps/web/src/lib/scraper-puppeteer.ts`:

```typescript
import puppeteer from 'puppeteer';

export async function scrapeWithPuppeteer(url: string) {
  const browser = await puppeteer.launch({
    headless: true,
    args: [
      '--no-sandbox',
      '--disable-setuid-sandbox',
      '--disable-dev-shm-usage',
      '--disable-gpu',
    ],
  });

  try {
    const page = await browser.newPage();
    
    // Set viewport
    await page.setViewport({ width: 1920, height: 1080 });
    
    // Set realistic user agent
    await page.setUserAgent(
      'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    );
    
    // Navigate to page
    await page.goto(url, {
      waitUntil: 'networkidle2',
      timeout: 30000,
    });
    
    // Wait for content to load
    await page.waitForTimeout(2000);
    
    // Scroll to load lazy images
    await autoScroll(page);
    
    // Extract data based on platform
    if (url.includes('airbnb.com')) {
      return await scrapeAirbnbWithPuppeteer(page, url);
    } else {
      return await scrapeGenericWithPuppeteer(page, url);
    }
    
  } finally {
    await browser.close();
  }
}

async function autoScroll(page: any) {
  await page.evaluate(async () => {
    await new Promise<void>((resolve) => {
      let totalHeight = 0;
      const distance = 100;
      const timer = setInterval(() => {
        const scrollHeight = document.body.scrollHeight;
        window.scrollBy(0, distance);
        totalHeight += distance;

        if (totalHeight >= scrollHeight) {
          clearInterval(timer);
          resolve();
        }
      }, 100);
    });
  });
}

async function scrapeAirbnbWithPuppeteer(page: any, url: string) {
  // Extract data from page
  const data = await page.evaluate(() => {
    // Extract from window.__INITIAL_STATE__ or DOM
    const getTextContent = (selector: string) => {
      const el = document.querySelector(selector);
      return el?.textContent?.trim() || '';
    };
    
    // Get all images
    const images = Array.from(document.querySelectorAll('img'))
      .map(img => img.src || img.getAttribute('data-original-uri'))
      .filter(src => 
        src && 
        !src.includes('icon') && 
        !src.includes('logo') &&
        (src.includes('airbnb.com/pictures') || src.startsWith('https://'))
      );
    
    // Get property details from visible text
    const bodyText = document.body.textContent || '';
    
    const extractNumber = (pattern: RegExp) => {
      const match = bodyText.match(pattern);
      return match ? parseInt(match[1]) : 0;
    };
    
    return {
      images: [...new Set(images)],
      guests: extractNumber(/(\d+)\s+guests?/i),
      bedrooms: extractNumber(/(\d+)\s+bedrooms?/i),
      beds: extractNumber(/(\d+)\s+beds?(?!\s*room)/i),
      bathrooms: extractNumber(/(\d+)\s+bathrooms?/i),
      name: getTextContent('h1'),
      location: getTextContent('[data-section-id="LOCATION_DEFAULT"] h2'),
    };
  });
  
  // Get amenities by clicking "Show all amenities" if available
  try {
    const showAllButton = await page.$('button:contains("Show all")');
    if (showAllButton) {
      await showAllButton.click();
      await page.waitForTimeout(1000);
      
      const amenities = await page.evaluate(() => {
        const amenityElements = document.querySelectorAll('[data-section-id="AMENITIES_DEFAULT"] li');
        return Array.from(amenityElements).map(el => el.textContent?.trim()).filter(Boolean);
      });
      
      data.amenities = amenities;
    }
  } catch (e) {
    // Button not found or couldn't click
  }
  
  return data;
}

async function scrapeGenericWithPuppeteer(page: any, url: string) {
  // Generic scraping logic
  return await page.evaluate(() => {
    return {
      name: document.querySelector('h1')?.textContent?.trim() || '',
      images: Array.from(document.querySelectorAll('img'))
        .map(img => img.src)
        .filter(src => src && src.startsWith('http')),
      // ... other fields
    };
  });
}
```

### Option 2: Serverless (Vercel/AWS Lambda)

Create `apps/web/src/lib/scraper-serverless.ts`:

```typescript
import chromium from 'chrome-aws-lambda';
import puppeteer from 'puppeteer-core';

export async function scrapeServerless(url: string) {
  const browser = await puppeteer.launch({
    args: chromium.args,
    executablePath: await chromium.executablePath,
    headless: chromium.headless,
  });

  // ... same logic as above
}
```

### Update API Route

Update `apps/web/src/app/api/scrape-property/route.ts`:

```typescript
import { scrapeWithPuppeteer } from '@/lib/scraper-puppeteer';
// or
import { scrapeServerless } from '@/lib/scraper-serverless';

export async function POST(request: NextRequest) {
  try {
    const { url, usePuppeteer } = await request.json();

    if (!url) {
      return NextResponse.json({ error: 'URL is required' }, { status: 400 });
    }

    let propertyData;

    // Use Puppeteer for better results (optional based on flag)
    if (usePuppeteer || url.includes('airbnb.com')) {
      propertyData = await scrapeWithPuppeteer(url);
    } else {
      // Fall back to Cheerio for faster scraping
      const html = await fetch(url).then(r => r.text());
      const $ = cheerio.load(html);
      propertyData = await scrapeAirbnb($, html, url);
    }

    return NextResponse.json({ success: true, data: propertyData });
  } catch (error: any) {
    console.error('Scraping error:', error);
    return NextResponse.json(
      { error: 'Failed to scrape property data', details: error.message },
      { status: 500 }
    );
  }
}
```

## Deployment Considerations

### Local/VPS Hosting
- Install Chrome/Chromium
- Sufficient memory (at least 1GB)
- Install Puppeteer normally

### Vercel/Netlify/AWS Lambda
- Use `chrome-aws-lambda` package
- Function timeout: 10-30 seconds
- Memory: 1024MB minimum
- Region: Choose closer to target sites

### Docker
```dockerfile
FROM node:18-slim

# Install Chrome
RUN apt-get update && apt-get install -y \
    chromium \
    && rm -rf /var/lib/apt/lists/*

ENV PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium

WORKDIR /app
COPY . .
RUN npm install
CMD ["npm", "start"]
```

## Performance Optimization

### 1. Block Unnecessary Resources
```typescript
await page.setRequestInterception(true);
page.on('request', (req) => {
  if (['image', 'stylesheet', 'font'].includes(req.resourceType())) {
    req.abort();
  } else {
    req.continue();
  }
});
```

Wait, don't block images if you need them!

### 2. Reuse Browser Instance
```typescript
let browser: any = null;

async function getBrowser() {
  if (!browser) {
    browser = await puppeteer.launch(/* ... */);
  }
  return browser;
}
```

### 3. Add Timeout
```typescript
const page = await browser.newPage();
page.setDefaultTimeout(15000); // 15 seconds
```

## Cost Comparison

### Cheerio (Current)
- **Speed**: 500ms - 1s per scrape
- **Memory**: ~50MB
- **Cost**: Negligible
- **Success Rate**: 70-80%

### Puppeteer (Upgrade)
- **Speed**: 3-5s per scrape
- **Memory**: ~200-300MB
- **Cost**: Higher compute costs
- **Success Rate**: 95%+

## When to Upgrade

Upgrade to Puppeteer when:
- âœ… You need all images (including lazy-loaded)
- âœ… Scraping success rate is too low
- âœ… Need to interact with the page (click buttons, etc.)
- âœ… Platform blocks basic HTTP requests
- âœ… Moving to production with paying users

Stick with Cheerio when:
- âœ… MVP/testing phase
- âœ… Cost-sensitive
- âœ… Current success rate is acceptable
- âœ… Low scraping volume

## Testing

Test locally:
```bash
node -e "
const { scrapeWithPuppeteer } = require('./lib/scraper-puppeteer');
scrapeWithPuppeteer('https://www.airbnb.com/rooms/12345678')
  .then(data => console.log(JSON.stringify(data, null, 2)))
  .catch(err => console.error(err));
"
```

## Monitoring

Add logging:
```typescript
console.log('[Scraper] Starting scrape:', url);
console.log('[Scraper] Found images:', images.length);
console.log('[Scraper] Extraction time:', Date.now() - startTime, 'ms');
```

Add error tracking (Sentry, LogRocket, etc.)

## Security

1. **Validate URLs**: Only allow known platforms
2. **Rate Limiting**: Prevent abuse
3. **Timeout**: Kill long-running scrapes
4. **Sandbox**: Run in isolated environment

```typescript
const ALLOWED_DOMAINS = ['airbnb.com', 'booking.com', 'vrbo.com'];

if (!ALLOWED_DOMAINS.some(domain => url.includes(domain))) {
  throw new Error('Unsupported platform');
}
```

## Summary

- **Current Solution**: Good for MVP with Airbnb
- **Puppeteer Upgrade**: When you need production reliability
- **Implementation Time**: 2-4 hours
- **Cost Impact**: 2-3x higher compute costs
- **Success Rate**: 70% â†’ 95%+

Ready to implement when you need it! ðŸš€


